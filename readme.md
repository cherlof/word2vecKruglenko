# Word2vec
## Данные: датасет qa_wiki_ru https://huggingface.co/datasets/AIR-Bench/qa_wiki_ru
### Задание
#### Подготовка данных:
• Соберите или найдите объемный текстовый корпус (например,
коллекции статей, книг или доменно-специфических текстов).
  
• Выполните предобработку текста (токенизация, приведение к нижнему
регистру, удаление пунктуации и стоп-слов при необходимости).  
##### Package textEdit.\_\_init__.py при помощи библиотеки nltk предобрабатывает датасет и сохраняет [preprocessed_large_file.csv](textEdit/preprocessed_large_file.csv).  
#### Обучение модели:
• С помощью библиотеки “gensim” обучите модели Word2Vec используя
архитектуры CBOW и Skip-gram на вашем корпусе.  

• Поэкспериментируйте с различными гиперпараметрами (размер
эмбеддинга, размер окна, минимальная частота и т. д.).  
  
##### Обучение происходит в Package fitModel.\_\_init__.py с различными гиперпараметрами такими как window, vector_size, sg(0-CBOW,1-SkipGram) и сохраняет модели в файлы. 

#### Анализ:
• Визуализируйте эмбеддинги слов с помощью методов PCA или t-SNE.
• Найдите наиболее похожие слова для заданного списка слов и
интерпретируйте результаты.
• Выполните задачи на аналогии слов (например, "король" - "мужчина" +
"женщина" ≈ ?).
mostSimilar/output.txt где в модели sg0 используется CBOW а sg1 SkipGram
![cbow_pca.png](showResult/cbow_pca.png)